{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsX9HcswkMhh"
      },
      "source": [
        "# Workshop 2.1 : web parsing with str functions\n",
        "We will crawl data from https://www.chula.ac.th/en/academics/faculties-and-schools/\n",
        "\n",
        "which is mirrored here https://comprogchula.github.io/\n",
        "\n",
        "<img src =\"https://github.com/Mixelon-tera/Workshop2_Datascraping_Resource/raw/master/source/intro_scraping.png\" width=\"800\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pkX1wP3kMhi"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "import urllib.request as urq\n",
        "import os\n",
        "\n",
        "dir_path = os.path.dirname(os.path.realpath(\"__file__\"))\n",
        "print(dir_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F59BwNdMieQw"
      },
      "source": [
        "### Tip! \n",
        " \n",
        "#### สำหรับเรื่อง string ในภาษา python ต้องระวังเรื่อง escape character เรามักจะใช้เครื่องหมาย \\ นำหน้า escape character ดังนี้\n",
        "\n",
        "<img src=\"https://i2.wp.com/www.techpaste.com/wp-content/uploads/2014/06/Escape_Characters_Python.jpg?fit=441%2C425&ssl=1\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a5vdLTmkMhl"
      },
      "source": [
        "# TO DO 1 : Crawl \"Faculty Name\" [only 19 faculties] (1%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGZp9qD8kMhm"
      },
      "source": [
        "Print all Faculty name in Chulalongkorn University\n",
        "and print Number of Faculties like below image\n",
        "\n",
        "<img src=\"https://github.com/Mixelon-tera/Workshop2_Datascraping_Resource/raw/master/source/fac_name.png\" width=300>\n",
        "\n",
        "<img src=\"https://github.com/Mixelon-tera/Workshop2_Datascraping_Resource/raw/master/source/number_faculty.png\" width=300>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF2I-cQFkMhn"
      },
      "outputs": [],
      "source": [
        "### บางเว็บไซต์เราไม่สามารถใช้ urlopen ได้ตรงๆ เนื่องจากเว็บมีการป้องกันไว้ วิธีการแก้เบื้องต้นคือการใส่ header เพื่อหลอกเว็บไซต์ว่าเรากำลังใช้เว็บในการ request ข้อมูลเพจ ###\n",
        "### แต่ในการบ้านวันนี้เราสามารถใส่ url ลงใน urlopen ได้ตรงๆเลย เพราะเว็บไม่ได้มีการป้องกันไว้ ###\n",
        "\n",
        "url = 'https://comprogchula.github.io/'\n",
        "html = str(urq.urlopen(url).read().decode('utf-8'))\n",
        "\n",
        "# ---- TO DO 1 : Code Here ----\n",
        "op,ed,a = [],[],[]\n",
        "for i in range(len(html)):\n",
        "    if(html[i:i+2] == \"<a\"):op+=[i]\n",
        "    if(html[i:i+2] == \"a>\"):ed+=[i+2]\n",
        "\n",
        "for i in range(len(op)):a+=[html[op[i]:ed[i]]]\n",
        "\n",
        "cnt=0\n",
        "for tag in a:\n",
        "    if(\"Faculty\" not in tag):continue\n",
        "    bracket,name=0,\"\"\n",
        "    cnt+=1\n",
        "    for i in tag:\n",
        "        if(i == '>' or i == '<'):bracket+=1\n",
        "        if(bracket==2 and i!='>'):name+=i\n",
        "    print(name)\n",
        "print(f\"Number of Faculties :  {cnt}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J3G2YYEkMhq"
      },
      "source": [
        "# TO DO 2 : Crawl \"Faculty and Schools Image\" [21 faculties and schools] (1%)\n",
        "\n",
        "Print image's url and save image into folder 'faculty_image'\n",
        "\n",
        "#### Hint : save images into -> dir_path+\"/faculty_image\"+faculty_name\n",
        "\n",
        "<img src=\"https://github.com/pjumruspun/ComProg2021-Workshop/blob/main/Workshop-02-Scraping/images.png?raw=true\" width=600>\n",
        "\n",
        "<img src=\"https://github.com/pjumruspun/ComProg2021-Workshop/blob/main/Workshop-02-Scraping/images_dir.png?raw=true\" width=600>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7XGFlJ972N3"
      },
      "outputs": [],
      "source": [
        "#สร้าง folder ชื่อ faculty_image\n",
        "\n",
        "!mkdir -p faculty_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6egHZ54tTMX2"
      },
      "source": [
        "### ขั้นตอนการอ่านและบันทึกไฟล์ภาพ\n",
        "\n",
        "1. อ่านภาพจากลิงค์\n",
        "* d = url.urlopen( [ลิงค์ของภาพ] )\n",
        "---\n",
        "2. สร้างไฟล์พร้อมระบุตำแหน่งที่จะเก็บไฟล์ภาพ \n",
        "* l = open( [ระบุตำแหน่งที่จะเก็บภาพ] )\n",
        "---\n",
        "\n",
        "3. บันทึกข้อมูลภาพไปยังตำแหน่งที่เก็บไฟล์ตามที่ระบุไว้ในข้อ (2.)\n",
        "* l.write(d.read())\n",
        "---\n",
        "\n",
        "4. ปิดไฟล์\n",
        "* l.close()\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JSrIjIHkMhq"
      },
      "outputs": [],
      "source": [
        "# ---- TO DO 2 : Code Here ----\n",
        "\n",
        "op = []\n",
        "for i in range(len(html)):\n",
        "    if(html[i:i+9] == \"data-src=\"):op+=[i+10]\n",
        "\n",
        "for op in op:\n",
        "    ed=op+10\n",
        "    while(html[ed]!='\"'):ed+=1\n",
        "    uri = html[op:ed]\n",
        "    if(\"aculty\" not in uri):continue\n",
        "\n",
        "    d = urq.urlopen(uri)\n",
        "    filename = uri[uri.rfind('/')+1:]\n",
        "    path = f'./faculty_image/{filename}'\n",
        "    I = open(path, 'wb')\n",
        "    I.write(d.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu9Lq8EYm_4O"
      },
      "source": [
        "# TO DO 3 : Crawl \"Faculty Telephone Number\" [19 Faculties] (1%)\n",
        "\n",
        "Print Tel. of each faculty in Chulalongkorn University\n",
        "\n",
        "*** Only Faculty ***\n",
        "\n",
        "<img src=\"https://github.com/pjumruspun/ComProg2021-Workshop/blob/main/Workshop-02-Scraping/tel.png?raw=true\" width=450>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CUSb9KvkMht"
      },
      "outputs": [],
      "source": [
        "# ---- TO DO 3 : Code Here ----\n",
        "url = 'https://comprogchula.github.io/'\n",
        "html = str(urq.urlopen(url).read().decode('utf-8'))\n",
        "\n",
        "op,ed,a = [],[],[]\n",
        "for i in range(len(html)):\n",
        "    if(html[i:i+2] == \"<a\"):op+=[i]\n",
        "    if(html[i:i+2] == \"a>\"):ed+=[i+2]\n",
        "\n",
        "for i in range(len(op)):a+=[html[op[i]:ed[i]]]\n",
        "\n",
        "uris = []\n",
        "for tag in a:\n",
        "    if(\"Faculty\" not in tag):continue\n",
        "    quote,uri=0,\"\"\n",
        "    for i in tag:\n",
        "        if(i == '\"'):quote+=1\n",
        "        elif(quote==1):uri+=i\n",
        "    uris+=[uri]\n",
        "\n",
        "for warp in uris:\n",
        "    print(warp[warp.find(\"faculty\"):warp.rfind(\".html\")])\n",
        "    html = str(urq.urlopen(warp).read().decode('utf-8'))\n",
        "    ok = False\n",
        "    for i in range(len(html)):\n",
        "        if(html[i:i+17] == \"content-contact-2\"):ok=True\n",
        "        \n",
        "        if(ok and html[i:i+13] == \"Tel:</strong>\"):\n",
        "            op,ed = i+13,i+23\n",
        "            while(html[op]!=\"+\"):op+=1\n",
        "            while(html[ed]!=\"<\"):ed+=1\n",
        "            \n",
        "            print(\"    \",html[op+1:ed])\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6qXu-BSkMhv"
      },
      "source": [
        "# TO DO 4: Crawl faculty of CP (1%)\n",
        "\n",
        "Current Faculty Members and Retired Faculty Members\n",
        "\n",
        "from https://www.cp.eng.chula.ac.th/en/about/faculty/\n",
        "\n",
        "*** Crawl only active members that have image ***\n",
        "\n",
        "**hint:** There are multiple strategies. To make it simple, we also accept a solution that list Dr. PITCHAYA as well. Another way is to check whether the image is the same filecmp.cmp https://docs.python.org/3/library/filecmp.html can do this. Or you can use if statements to exclude known exceptions.\n",
        "\n",
        "<img src=\"https://github.com/thcktw/Workshop2_Datascraping_Resource/raw/master/source/crawl_fac_member.png\" width=300>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_pattern(bik, smol, smol_ed):\n",
        "    a=[]\n",
        "    for i in range(len(bik)):\n",
        "        if(bik[i:i+len(smol)] == smol):\n",
        "            ed = i+len(smol)\n",
        "            while(html[ed:ed+len(smol_ed)] != smol_ed):ed+=1\n",
        "            a+=[(i,ed+1)]\n",
        "    return a\n",
        "\n",
        "def extract(data):\n",
        "    a={}\n",
        "    for i in range(len(data)):\n",
        "        if(data[i:i+14] == '<img src=\"http'):\n",
        "            op=i\n",
        "            while(data[op]!='\"'):op+=1\n",
        "            ed=op+10\n",
        "            while(data[ed]!='\"'):ed+=1\n",
        "            a['img_uri'] = data[op+1:ed].replace(\"amp;\", \"\")\n",
        "    \n",
        "    name_op,name_ed = find_pattern(data, \"<p>\", \"</a></p>\")[0]\n",
        "    name = data[name_op+3:name_ed]\n",
        "    \n",
        "    sttime = True\n",
        "    space_cnt = 0\n",
        "    cleaned = \"\"\n",
        "    for i in name:\n",
        "        if(space_cnt >=5 ): break\n",
        "        if(i == \" \" and sttime): continue\n",
        "        sttime=False\n",
        "        if(i == \" \"): space_cnt+=1\n",
        "        cleaned+=i\n",
        "\n",
        "    # print(cleaned)\n",
        "    a['name'] = cleaned\n",
        "\n",
        "    return a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "DSh-mDUckMhw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Faculty Members\n",
            "\n",
            "PROF. DR. BOONSERM KIJSIRIKUL  \n",
            "PROF. DR. PRABHAS CHONGSTITVATANA  \n",
            "ASSOC. PROF. DR. ATHASIT SURARERKS \n",
            "ASSOC. PROF. DR. ATIWONG SUCHATO \n",
            "ASSOC. PROF. DR. CHOTIRAT RATANAMAHATANA \n",
            "ASSOC. PROF. DR. DUANGDAO  \n",
            "ASSOC. PROF. DR. KRERK PIROMSOPA \n",
            "ASSOC. PROF. DR. KULTIDA ROJVIBOONCHAI \n",
            "ASSOC. PROF. DR. NUTTAPONG CHENTANEZ \n",
            "ASSOC. PROF. DR. PEERAPON VATEEKUL \n",
            "ASSOC. PROF. DR. PROADPRAN PUNYABUKKANA \n",
            "ASSOC. PROF. DR. SETHA PAN-NGUM \n",
            "ASSOC. PROF. DR. TARATIP SUWANNASART \n",
            "ASSOC. PROF. DR. THANARAT CHALIDABHONGSE \n",
            "ASSOC. PROF. DR. TWITTIE SENIVONGSE \n",
            "ASSOC. PROF. DR. VISHNU KOTRAJARAS \n",
            "ASSOC. PROF. DR. WIWAT VATANAWOOD \n",
            "ASSOC. PROF. DR. YACHAI LIMPIYAKORN \n",
            "ASSOC. PROF. NAKORNTHIP PROMPOON  \n",
            "ASST. PROF. DR. ARTHIT THONGTAK \n",
            "ASST. PROF. DR. ATTAWITH SUDSANG \n",
            "ASST. PROF. DR. KUNWADEE  \n",
            "ASST. PROF. DR. NATAWUT NUPAIROJ \n",
            "ASST. PROF. DR. NATTEE NIPARNAN \n",
            "ASST. PROF. DR. PIZZANU KANONGCHAIYOS \n",
            "ASST. PROF. DR. SUKREE SINTHUPINYO \n",
            "ASST. PROF. DR. VEERA MUANGSIN \n",
            "ASST. PROF. CHATE PATANOTHAI  \n",
            "DR. EKAPOL  CHUANGSUWANICH  \n",
            "DR. JESSADA THUTKAWKORAPIN   \n",
            "DR. NUENGWONG TUAYCHAROEN   \n",
            "DR. PITTIPOL  KANTAVAT  \n",
            "DR. PUNNARAI  SIRICHAROEN  \n",
            "THONGCHAI ROJAKANGSADAN    \n",
            "\n",
            "Retired Faculty Members\n",
            "\n",
            "ASSOC. PROF. DR. PORNSIRI MUENCHAISRI \n",
            "Assc.Prof. Dr. SARTID VONGPRADHIP  \n",
            "ASSOC. PROF. DR. SOMCHAI PRASITJUTRAKUL \n",
            "Assc.Prof. Dr. WANCHAI RIVEPIBOON  \n",
            "Assc.Prof. MANDHANA PRAKANSAMUT   \n",
            "Asst.Prof. Dr. SUEBSKUL PHIPHOBMONGKOL  \n",
            "Asst.Prof. BOONCHAI SOWANWANICHAKUL   \n",
            "Asst.Prof. Korbkul Tejavanija   \n",
            "Asst.Prof. THANAWAN CHANTARATANAPIBUL   \n",
            "Dr. YUNYONG TENG-AMNUAY   \n"
          ]
        }
      ],
      "source": [
        "# ---- TO DO 4 : Code Here ----\n",
        "import filecmp\n",
        "\n",
        "\n",
        "url = \"https://www.cp.eng.chula.ac.th/en/about/faculty/\"\n",
        "html = str(urq.urlopen(url).read().decode('utf-8'))\n",
        "\n",
        "instructor = find_pattern(html, '<tr class=\"instructorRow\"', \"</tr>\")\n",
        "c=0;print(\"Current Faculty Members\\n\")\n",
        "for op,ed in instructor:    \n",
        "    data = html[op:ed].replace('\\n','')\n",
        "    info = extract(data)\n",
        "    #print(info, '\\n')\n",
        "    \n",
        "    if(\"Interest\" in info['name']): continue\n",
        "\n",
        "    # see if the picture in none\n",
        "\n",
        "    d = urq.urlopen(info['img_uri'])\n",
        "    path = './professor/check.jpg'\n",
        "    I = open(path, 'wb')\n",
        "    I.write(d.read())\n",
        "\n",
        "    comp = filecmp.cmp(path, './professor/none.jpg')\n",
        "    if(comp):continue\n",
        "\n",
        "    print(info['name'])\n",
        "    c+=1\n",
        "    if(c==34):print(\"\\nRetired Faculty Members\\n\")\n",
        "    if(c==44):break\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmL5uX5RWp2f"
      },
      "source": [
        "# To Do 5\n",
        "\n",
        "ทดลองดึงข้อมูลจาก dek-d.com ดูสิ :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "6r8NQmAYkMhy"
      },
      "outputs": [],
      "source": [
        "dd_url = 'https://www.dek-d.com/home/writer/'\n",
        "nh_url = 'https://nhentai.net/api/galleries/search?sort=popular&query=uploaded:%3C1d'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAFkUJU0YCaF"
      },
      "source": [
        "ลองรันโค๊ดบรรทัดด้านล่างดู"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVrU9jG0XjvF"
      },
      "outputs": [],
      "source": [
        "html = str(urq.urlopen(dd_url).read().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9G_ZbZsYMUh"
      },
      "source": [
        "จะพบว่าเกิด error ขึ้นเนื่องจากเว็บ dek-d มีการป้องกันไว้ \n",
        "\n",
        "หากเราต้อง scrape จะต้องส่ง user-agent เช่น Web-browser, Accept-Charset, Accept-Encoding เป็นต้น เพื่อหลอกเว็บไซต์ว่าเรากำลังใช้เว็บในการ request ข้อมูลเพจ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeVk4zsWZLZ0"
      },
      "outputs": [],
      "source": [
        "headers={'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'} \n",
        "\n",
        "\n",
        "dd_request = urq.Request(dd_url,None,headers) \n",
        "dd_html = str(urq.urlopen(dd_request).read().decode('utf-8'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNO0U9aYf0Id"
      },
      "source": [
        "ลองดึงข้อมูล title นิยายจากเว็บ https://www.dek-d.com/home/writer/ ดูสิ!!!\n",
        "\n",
        "\n",
        "#### ตัวอย่าง output \n",
        "\n",
        "<img src=\"https://github.com/Mixelon-tera/Workshop2_Datascraping_Resource/raw/master/source/dek_d_fiction.png\" width=400>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "OQjDoNJKXESX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "เกิดอีกครั้งในยุค 80 ชาตินี้ไม่ใหญ่ไม่ได้\n",
            "เกิดอีกครั้งในยุค 80 ชาตินี้ไม่ใหญ่ไม่ได้\n",
            "เกิดอีกครั้งในยุค 80 ชาตินี้ไม่ใหญ่ไม่ได้\n",
            "鬼怪 นางมาร (ปิดตอนวันที่ 1 กันยายน)\n",
            "(อ่านฟรีทุกวัน) ทะลุมิติไปเป็นแพทย์หญิงชนบทตัวน้อยๆ : ความมั่งคั่งร่ำรวยมาถึงประตูของท่านแล้ว [นิยายแปล]\n",
            "หลี่อี้เฟย\n",
            "(อ่านฟรีทุกวัน วันละ 2 ตอน) เกิดใหม่ชาตินี้ ขอเป็นเศรษฐีนีในยุค 80 [นิยายแปล]\n",
            "(อ่านฟรีทุกวัน วันละ 2 ตอน) คู่มือเศรษฐีนีชาวนาฉบับสาวน้อยทะลุมิติ [นิยายแปล]\n",
            "ทะลุมิติมาเป็นแม่ลูกสองในยุค1978\n",
            "จ้าวลู่ชิง คหบดีหญิงแห่งต้าถัง\n",
            "ใครส่งข้ามาป่วน\n",
            "พฤกษาสวรรค์หวนคืน\n",
            "(อ่านฟรีทุกวัน) ทะลุมิติไปเป็นภรรยาชาวสวนของท่านบัณฑิต [นิยายแปล]\n",
            "นางร้ายทะลุมิติไปเป็นคุณแม่ลูกสี่ยุค 70\n",
            "สาวใช้อันดับหนึ่ง\n",
            "ผักวิญญาณพวกนี้ฉันเป็นคนปลูก\n",
            "(อ่านฟรีทุกวัน วันละ 2 ตอน) ภรรยานายพรานตัวน้อยกับระบบร้านค้ามือสอง [นิยายแปล]\n",
            "[อ่านฟรีทุกวัน] ทะลุมิติไปเป็นสาวนาผู้ร่ำรวย [ 农女致富记 ]\n",
            "หนูน้อยสกุลถัง\n",
            "หมอหญิงทะลุมิติกับตำราวิเศษ\n",
            "(อ่านฟรีทุกวัน วันละ 2 ตอน) ทะลุมิติไปทำฟาร์มกับหมอหญิงตัวน้อย [นิยายแปล]\n",
            "ภารกิจพิชิตใจสามีในยุคปลาย70 (รีบอ่านก่อนติดเหรียญวันที่ 1 กันยายนนี้นะคะ)\n"
          ]
        }
      ],
      "source": [
        "# ---- OPTIONAL : Code Here ----\n",
        "\n",
        "headers={'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'} \n",
        "\n",
        "\n",
        "dd_request = urq.Request(dd_url,None,headers) \n",
        "dd_html = str(urq.urlopen(dd_request).read().decode('utf-8'))\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "soup = BeautifulSoup(dd_html, \"html.parser\")\n",
        "\n",
        "a = soup.find_all(\"a\", class_=\"card__title\")\n",
        "for i in a:\n",
        "    print(i.text)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "00_web_scraping_with_str_functions_2021",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
